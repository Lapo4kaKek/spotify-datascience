{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "luenf50xv0bv2ql9j6mm19",
    "id": "tBrDXMdDy-Qn"
   },
   "source": [
    "# HSE 2023: Введение в машинное обучение БИ 23/24\n",
    "\n",
    "## ДЗ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "l9vanmd2bvdcly1cxhnbc5",
    "id": "RXXi5K1mf41d"
   },
   "source": [
    "# Внимание!\n",
    "Если в задании просят объяснить что-либо, то это значит, что требуется письменный ответ, который является частью задания и оценивается\n",
    "\n",
    "Мы только принимаем ipynb ноутбуки. Если вы используете Google Colab, то вам необходимо скачать ноутбук перед сдачей ДЗ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T16:48:20.566549Z",
     "start_time": "2020-09-26T16:48:19.893995Z"
    },
    "cellId": "al1tp3cw5j2dqf6rvsez7",
    "id": "mSR-a9vVy-Qp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "# from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLSResults\n",
    "from math import sqrt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "p5t1m2565ona8fyci1kdl",
    "id": "HUjuv9Qty-Qq"
   },
   "source": [
    "### Данные\n",
    "\n",
    "Для этого ДЗ мы будем использовать датасет треков со стримингового сервиса Spotify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "gb778g3ch0983okrgrs5fc"
   },
   "source": [
    "**Описание данных**\n",
    "\n",
    "- **track_id:** The Spotify ID for the track\n",
    "- **artists:** The artists' names who performed the track. If there is more than one artist, they are separated by a ;\n",
    "- **album_name:** The album name in which the track appears\n",
    "- **track_name:** Name of the track\n",
    "- **popularity:** The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity is derived mathematically from track popularity.\n",
    "- **duration_ms:** The track length in milliseconds\n",
    "- **explicit:** Whether or not the track has explicit lyrics (true = yes it does; false = no it does not OR unknown)\n",
    "- **danceability:** Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable\n",
    "- **key:** The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1\n",
    "- **loudness:** The overall loudness of a track in decibels (dB)\n",
    "- **mode:** Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0\n",
    "- **speechiness:** Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks\n",
    "- **acousticness:** A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic\n",
    "- **instrumentalness:** Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content\n",
    "- **liveness:** Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live\n",
    "- **valence:** A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)\n",
    "- **tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration\n",
    "- **time_signature:** An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of 3/4, to 7/4.\n",
    "- **track_genre:** The genre in which the track belongs\n",
    "\n",
    "**Целевая переменная**\n",
    "- **energy:** Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "dbdzyaiojggqs6s5g59i9b",
    "id": "tHWSWTXDy-Qq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "y = data['energy']\n",
    "X = data.drop(['energy', 'artists', 'album_name', 'track_name'], axis=1)\n",
    "columns = X.columns\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "hooz725cyoiqd8m82cz2jh",
    "execution_id": "061e06cd-4709-42b8-b500-869f2d09d83b",
    "id": "K81w8s35y-Qq"
   },
   "source": [
    "## Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "tavi0tzoln6p9nmguhcg7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "doin22ub9gpmctqhnezqta",
    "execution_id": "76098532-cf5a-4d70-a568-e45a7b8d539f",
    "id": "cYgEN-FMy-Qr"
   },
   "source": [
    "#### 0. [0.25 балла] Закодируйте категориальные признаки. Объясните выбранный вами метод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "2gp4lcdzqfm4nv2c1lwpio"
   },
   "outputs": [],
   "source": [
    "X['explicit'] = X['explicit'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "saj297wx3ex3ev2hwe1ns",
    "id": "-IrSlQaWy-Qr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# ваш код здесь \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "# One-Hot Encoding для категориальных признаков\n",
    "X_encoded = pd.get_dummies(X, columns=['track_genre'])\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "cellId": "gkiet0u9bpnq9uuewptqd",
    "id": "7dVwP45Gy-Qr"
   },
   "outputs": [],
   "source": [
    "# 1. [0.25 балла] Разбейте данные на train и test с пропорцией 75:25 и random_state=7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "ilwjvkfg82ni3ekq4ckcs8",
    "id": "U7z8TIh5y-Qs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# ваш код здесь \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "# X_encoded = pd.get_dummies(X, columns=['artists', 'album_name', 'track_genre'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(pd.concat((X.drop(columns=['track_genre']), X_encoded), axis=1), y, test_size=0.25, random_state=7)\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "5bu6uazl1nolnlj7htboqn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85500, 142)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellId": "yvttbzeuvpb9gbfvnqccnk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popularity                   int64\n",
      "duration_ms                  int64\n",
      "explicit                     int64\n",
      "danceability               float64\n",
      "key                          int64\n",
      "                            ...   \n",
      "track_genre_techno           uint8\n",
      "track_genre_trance           uint8\n",
      "track_genre_trip-hop         uint8\n",
      "track_genre_turkish          uint8\n",
      "track_genre_world-music      uint8\n",
      "Length: 142, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ujkdccvqq3a6tcykzfnwcj",
    "execution_id": "cd837ab4-e76e-49ea-9554-91854e47d511",
    "id": "7daIQRfKy-Qs",
    "tags": []
   },
   "source": [
    "#### 2. [0.75 балла] Обучите модели на train'е, исключив категориальные признаки, используя библиотеку StatsModels и примените ее к test'у; используйте $RMSE$ и $R ^ 2$ в качестве метрики качества. Попробуйте также применить реализации линейной регрессии из sklearn:\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.03$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.05$\n",
    "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.4$\n",
    "\n",
    "Не забывайте скейлить данные с помощью StandardScaler перед обучением моделей! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "ggbcq59tad9bdb2ih9r5gs",
    "id": "Bkbr5iFCy-Qs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "vxkpq04akjgl5o8j9hqmo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.01481095148096353\n",
      "R^2: 0.7646687020704666\n"
     ]
    }
   ],
   "source": [
    "X = data.drop('energy', axis=1) \n",
    "y = data['energy']\n",
    "\n",
    "# Удаление столбца с названием трека\n",
    "X.drop('track_name', axis=1, inplace=True)\n",
    "\n",
    "# Преобразование столбца explicit в числовой формат\n",
    "X['explicit'] = X['explicit'].astype(int)\n",
    "\n",
    "# Определение нечисловых столбцов\n",
    "non_numeric_columns = X.select_dtypes(exclude=['float64', 'int64']).columns\n",
    "\n",
    "# Преобразование нечисловых столбцов с помощью LabelEncoder\n",
    "for col in non_numeric_columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Масштабирование данных\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Обучение модели линейной регрессии\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Предсказания модели\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Расчет метрик\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R^2: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "a0zv1a05n664fceb9tsfig"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91200, 17)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "224z375b6yozqcasaz8xd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 0.12170025259202846\n",
      "Linear Regression R^2: 0.7646687020704666\n",
      "-----------\n",
      "Ridge Regression RMSE: 0.12170025363233108\n",
      "Ridge Regression R^2: 0.7646686980472084\n",
      "-----------\n",
      "Lasso Regression RMSE: 0.14798373613042706\n",
      "Lasso Regression R^2: 0.6520436982694859\n",
      "-----------\n",
      "ElasticNet Regression RMSE: 0.12268881141903015\n",
      "ElasticNet Regression R^2: 0.7608300299609407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model = Ridge(alpha=0.03)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso_model = Lasso(alpha=0.05)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "y_pred_lasso = lasso_model.predict(X_test_scaled)\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "# ElasticNet Regression\n",
    "elastic_model = ElasticNet(alpha=0.01, l1_ratio=0.4)\n",
    "elastic_model.fit(X_train_scaled, y_train)\n",
    "y_pred_elastic = elastic_model.predict(X_test_scaled)\n",
    "rmse_elastic = np.sqrt(mean_squared_error(y_test, y_pred_elastic))\n",
    "r2_elastic = r2_score(y_test, y_pred_elastic)\n",
    "\n",
    "# Вывод результатов\n",
    "print(f\"Linear Regression RMSE: {rmse_lr}\")\n",
    "print(f\"Linear Regression R^2: {r2_lr}\")\n",
    "print(\"-----------\")\n",
    "print(f\"Ridge Regression RMSE: {rmse_ridge}\")\n",
    "print(f\"Ridge Regression R^2: {r2_ridge}\")\n",
    "print(\"-----------\")\n",
    "print(f\"Lasso Regression RMSE: {rmse_lasso}\")\n",
    "print(f\"Lasso Regression R^2: {r2_lasso}\")\n",
    "print(\"-----------\")\n",
    "print(f\"ElasticNet Regression RMSE: {rmse_elastic}\")\n",
    "print(f\"ElasticNet Regression R^2: {r2_elastic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xuq5uidsb47od17kecq58",
    "execution_id": "f14fa7e3-6881-4de6-bdc7-76169a85e484"
   },
   "source": [
    "#### 3. [0.25 балла] Повторите шаги из предыдущего пункта, добавив категориальные признаки. Прокомментируйте изменения значений метрик качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "cellId": "luwpipc577iqe28y83yi3l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "oj5gn0oh8qt9vgtyq6so"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with One-Hot Encoding): 0.012731919140719713\n",
      "R^2 (with One-Hot Encoding): 0.7977024595367508\n"
     ]
    }
   ],
   "source": [
    "# Разделение данных на признаки и целевую переменную\n",
    "X = data.drop(['energy', 'track_name', 'artists', 'album_name'], axis=1) \n",
    "\n",
    "# Преобразование столбца explicit в числовой формат\n",
    "X['explicit'] = X['explicit'].astype(int)\n",
    "\n",
    "# One-Hot Encoding для категориальных признаков\n",
    "X_encoded = pd.get_dummies(X, columns=['track_genre'])\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(pd.concat((X.drop(columns=['track_genre']), X_encoded), axis=1), data['energy'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Масштабирование данных\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Обучение модели линейной регрессии\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Предсказания модели\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Расчет метрик\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'MSE (with One-Hot Encoding): {mse}')\n",
    "print(f'R^2 (with One-Hot Encoding): {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellId": "v5ga1njulx991akmdr1zvs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91200, 142)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "zqeuvwizykhfxwy0rgj7",
    "execution_id": "9ab6c3bb-a4b5-4d3e-b9d0-28d3cd3bbad4",
    "id": "69JOftKRy-Qt"
   },
   "source": [
    "#### 4. [1 балл] Исследуйте значения параметров полученных моделей и проверьте какие веса получились нулевыми. Прокомментируйте значимость коэффициентов, обшую значимость модели и остальные факторы из результирующей таблицы "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellId": "4lzq5vb9dydx1imaka2ok",
    "id": "Np1biYQ7y-Qt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a god\n"
     ]
    }
   ],
   "source": [
    "print(\"I am a god\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 energy   R-squared:                       0.801\n",
      "Model:                            OLS   Adj. R-squared:                  0.801\n",
      "Method:                 Least Squares   F-statistic:                     2885.\n",
      "Date:                Tue, 24 Oct 2023   Prob (F-statistic):               0.00\n",
      "Time:                        00:42:23   Log-Likelihood:                 70005.\n",
      "No. Observations:               91200   AIC:                        -1.398e+05\n",
      "Df Residuals:                   91072   BIC:                        -1.385e+05\n",
      "Df Model:                         127                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.6409      0.000   1716.746      0.000       0.640       0.642\n",
      "x1          -5.72e+09    7.3e+09     -0.784      0.433      -2e+10    8.58e+09\n",
      "x2          9.986e+10   6.56e+10      1.522      0.128   -2.88e+10    2.28e+11\n",
      "x3          1.469e+10   1.92e+10      0.763      0.445    -2.3e+10    5.24e+10\n",
      "x4         -2.246e+09    3.6e+09     -0.624      0.533    -9.3e+09    4.81e+09\n",
      "x5          1.081e+10   1.32e+10      0.822      0.411    -1.5e+10    3.66e+10\n",
      "x6           6.23e+08   2.66e+09      0.234      0.815    -4.6e+09    5.85e+09\n",
      "x7         -7.711e+09   5.28e+09     -1.460      0.144   -1.81e+10    2.64e+09\n",
      "x8          9.126e+09   2.16e+10      0.422      0.673   -3.33e+10    5.15e+10\n",
      "x9         -1.779e+10   1.42e+10     -1.252      0.211   -4.57e+10    1.01e+10\n",
      "x10        -1.836e+09   3.36e+09     -0.545      0.585   -8.43e+09    4.76e+09\n",
      "x11         1.228e+10   9.15e+09      1.342      0.180   -5.66e+09    3.02e+10\n",
      "x12         1.558e+10   2.07e+10      0.752      0.452    -2.5e+10    5.62e+10\n",
      "x13         1.905e+11   7.19e+10      2.647      0.008    4.94e+10    3.31e+11\n",
      "x14        -5.653e+09    3.7e+10     -0.153      0.878   -7.81e+10    6.68e+10\n",
      "x15          5.72e+09    7.3e+09      0.784      0.433   -8.58e+09       2e+10\n",
      "x16        -9.986e+10   6.56e+10     -1.522      0.128   -2.28e+11    2.88e+10\n",
      "x17        -1.469e+10   1.92e+10     -0.763      0.445   -5.24e+10     2.3e+10\n",
      "x18         2.246e+09    3.6e+09      0.624      0.533   -4.81e+09     9.3e+09\n",
      "x19        -1.081e+10   1.32e+10     -0.822      0.411   -3.66e+10     1.5e+10\n",
      "x20         -6.23e+08   2.66e+09     -0.234      0.815   -5.85e+09     4.6e+09\n",
      "x21         7.711e+09   5.28e+09      1.460      0.144   -2.64e+09    1.81e+10\n",
      "x22        -9.126e+09   2.16e+10     -0.422      0.673   -5.15e+10    3.33e+10\n",
      "x23         1.779e+10   1.42e+10      1.252      0.211   -1.01e+10    4.57e+10\n",
      "x24         1.836e+09   3.36e+09      0.545      0.585   -4.76e+09    8.43e+09\n",
      "x25        -1.228e+10   9.15e+09     -1.342      0.180   -3.02e+10    5.66e+09\n",
      "x26        -1.558e+10   2.07e+10     -0.752      0.452   -5.62e+10     2.5e+10\n",
      "x27        -1.905e+11   7.19e+10     -2.647      0.008   -3.31e+11   -4.94e+10\n",
      "x28         5.653e+09    3.7e+10      0.153      0.878   -6.68e+10    7.81e+10\n",
      "x29         4.644e+09   7.37e+09      0.631      0.528   -9.79e+09    1.91e+10\n",
      "x30         4.674e+09   7.41e+09      0.631      0.528   -9.85e+09    1.92e+10\n",
      "x31         4.639e+09   7.36e+09      0.631      0.528   -9.78e+09    1.91e+10\n",
      "x32         4.728e+09    7.5e+09      0.631      0.528   -9.97e+09    1.94e+10\n",
      "x33         4.691e+09   7.44e+09      0.631      0.528   -9.89e+09    1.93e+10\n",
      "x34         4.703e+09   7.46e+09      0.631      0.528   -9.92e+09    1.93e+10\n",
      "x35         4.653e+09   7.38e+09      0.631      0.528   -9.81e+09    1.91e+10\n",
      "x36         4.668e+09    7.4e+09      0.631      0.528   -9.84e+09    1.92e+10\n",
      "x37         4.642e+09   7.36e+09      0.631      0.528   -9.79e+09    1.91e+10\n",
      "x38         4.691e+09   7.44e+09      0.631      0.528   -9.89e+09    1.93e+10\n",
      "x39         4.685e+09   7.43e+09      0.631      0.528   -9.88e+09    1.92e+10\n",
      "x40         4.642e+09   7.36e+09      0.631      0.528   -9.79e+09    1.91e+10\n",
      "x41         4.703e+09   7.46e+09      0.631      0.528   -9.92e+09    1.93e+10\n",
      "x42         4.665e+09    7.4e+09      0.631      0.528   -9.84e+09    1.92e+10\n",
      "x43         4.642e+09   7.36e+09      0.631      0.528   -9.79e+09    1.91e+10\n",
      "x44         4.688e+09   7.44e+09      0.631      0.528   -9.89e+09    1.93e+10\n",
      "x45         4.688e+09   7.44e+09      0.631      0.528   -9.89e+09    1.93e+10\n",
      "x46         4.708e+09   7.47e+09      0.631      0.528   -9.93e+09    1.93e+10\n",
      "x47         4.708e+09   7.47e+09      0.631      0.528   -9.93e+09    1.93e+10\n",
      "x48         4.659e+09   7.39e+09      0.631      0.528   -9.82e+09    1.91e+10\n",
      "x49         4.665e+09    7.4e+09      0.631      0.528   -9.84e+09    1.92e+10\n",
      "x50         4.671e+09   7.41e+09      0.631      0.528   -9.85e+09    1.92e+10\n",
      "x51         4.705e+09   7.46e+09      0.631      0.528   -9.92e+09    1.93e+10\n",
      "x52         4.723e+09   7.49e+09      0.631      0.528   -9.96e+09    1.94e+10\n",
      "x53           4.7e+09   7.45e+09      0.631      0.528   -9.91e+09    1.93e+10\n",
      "x54           4.7e+09   7.45e+09      0.631      0.528   -9.91e+09    1.93e+10\n",
      "x55         4.685e+09   7.43e+09      0.631      0.528   -9.88e+09    1.92e+10\n",
      "x56           4.7e+09   7.45e+09      0.631      0.528   -9.91e+09    1.93e+10\n",
      "x57         4.671e+09   7.41e+09      0.631      0.528   -9.85e+09    1.92e+10\n",
      "x58         4.606e+09   7.31e+09      0.631      0.528   -9.71e+09    1.89e+10\n",
      "x59         4.682e+09   7.43e+09      0.631      0.528   -9.87e+09    1.92e+10\n",
      "x60         4.662e+09   7.39e+09      0.631      0.528   -9.83e+09    1.92e+10\n",
      "x61         4.682e+09   7.43e+09      0.631      0.528   -9.87e+09    1.92e+10\n",
      "x62         4.703e+09   7.46e+09      0.631      0.528   -9.92e+09    1.93e+10\n",
      "x63         4.647e+09   7.37e+09      0.631      0.528    -9.8e+09    1.91e+10\n",
      "x64         4.668e+09    7.4e+09      0.631      0.528   -9.84e+09    1.92e+10\n",
      "x65           4.7e+09   7.45e+09      0.631      0.528   -9.91e+09    1.93e+10\n",
      "x66          4.63e+09   7.34e+09      0.631      0.528   -9.76e+09     1.9e+10\n",
      "x67         4.705e+09   7.46e+09      0.631      0.528   -9.92e+09    1.93e+10\n",
      "x68         4.694e+09   7.44e+09      0.631      0.528    -9.9e+09    1.93e+10\n",
      "x69         4.647e+09   7.37e+09      0.631      0.528    -9.8e+09    1.91e+10\n",
      "x70         4.615e+09   7.32e+09      0.631      0.528   -9.73e+09     1.9e+10\n",
      "x71         4.606e+09   7.31e+09      0.631      0.528   -9.71e+09    1.89e+10\n",
      "x72         4.647e+09   7.37e+09      0.631      0.528    -9.8e+09    1.91e+10\n",
      "x73         4.677e+09   7.42e+09      0.631      0.528   -9.86e+09    1.92e+10\n",
      "x74         4.714e+09   7.48e+09      0.631      0.528   -9.94e+09    1.94e+10\n",
      "x75         4.685e+09   7.43e+09      0.631      0.528   -9.88e+09    1.92e+10\n",
      "x76         4.726e+09   7.49e+09      0.631      0.528   -9.96e+09    1.94e+10\n",
      "x77         4.668e+09    7.4e+09      0.631      0.528   -9.84e+09    1.92e+10\n",
      "x78         4.653e+09   7.38e+09      0.631      0.528   -9.81e+09    1.91e+10\n",
      "x79         4.688e+09   7.44e+09      0.631      0.528   -9.89e+09    1.93e+10\n",
      "x80         4.677e+09   7.42e+09      0.631      0.528   -9.86e+09    1.92e+10\n",
      "x81         4.668e+09    7.4e+09      0.631      0.528   -9.84e+09    1.92e+10\n",
      "x82         4.594e+09   7.29e+09      0.631      0.528   -9.69e+09    1.89e+10\n",
      "x83         4.633e+09   7.35e+09      0.631      0.528   -9.77e+09     1.9e+10\n",
      "x84         4.677e+09   7.42e+09      0.631      0.528   -9.86e+09    1.92e+10\n",
      "x85         4.647e+09   7.37e+09      0.631      0.528    -9.8e+09    1.91e+10\n",
      "x86         4.656e+09   7.38e+09      0.631      0.528   -9.82e+09    1.91e+10\n",
      "x87          4.65e+09   7.38e+09      0.631      0.528   -9.81e+09    1.91e+10\n",
      "x88         4.737e+09   7.51e+09      0.631      0.528   -9.99e+09    1.95e+10\n",
      "x89         4.644e+09   7.37e+09      0.631      0.528   -9.79e+09    1.91e+10\n",
      "x90         4.711e+09   7.47e+09      0.631      0.528   -9.93e+09    1.94e+10\n",
      "x91         4.612e+09   7.31e+09      0.631      0.528   -9.73e+09    1.89e+10\n",
      "x92         4.694e+09   7.44e+09      0.631      0.528    -9.9e+09    1.93e+10\n",
      "x93         4.644e+09   7.37e+09      0.631      0.528   -9.79e+09    1.91e+10\n",
      "x94          4.65e+09   7.38e+09      0.631      0.528   -9.81e+09    1.91e+10\n",
      "x95         4.703e+09   7.46e+09      0.631      0.528   -9.92e+09    1.93e+10\n",
      "x96         4.668e+09    7.4e+09      0.631      0.528   -9.84e+09    1.92e+10\n",
      "x97         4.697e+09   7.45e+09      0.631      0.528    -9.9e+09    1.93e+10\n",
      "x98         4.671e+09   7.41e+09      0.631      0.528   -9.85e+09    1.92e+10\n",
      "x99         4.668e+09    7.4e+09      0.631      0.528   -9.84e+09    1.92e+10\n",
      "x100        4.714e+09   7.48e+09      0.631      0.528   -9.94e+09    1.94e+10\n",
      "x101        4.688e+09   7.44e+09      0.631      0.528   -9.89e+09    1.93e+10\n",
      "x102        4.671e+09   7.41e+09      0.631      0.528   -9.85e+09    1.92e+10\n",
      "x103         4.72e+09   7.49e+09      0.631      0.528   -9.95e+09    1.94e+10\n",
      "x104        4.711e+09   7.47e+09      0.631      0.528   -9.93e+09    1.94e+10\n",
      "x105        4.737e+09   7.51e+09      0.631      0.528   -9.99e+09    1.95e+10\n",
      "x106        4.682e+09   7.43e+09      0.631      0.528   -9.87e+09    1.92e+10\n",
      "x107        4.671e+09   7.41e+09      0.631      0.528   -9.85e+09    1.92e+10\n",
      "x108        4.691e+09   7.44e+09      0.631      0.528   -9.89e+09    1.93e+10\n",
      "x109        4.723e+09   7.49e+09      0.631      0.528   -9.96e+09    1.94e+10\n",
      "x110        4.656e+09   7.38e+09      0.631      0.528   -9.82e+09    1.91e+10\n",
      "x111        4.662e+09   7.39e+09      0.631      0.528   -9.83e+09    1.92e+10\n",
      "x112        4.746e+09   7.53e+09      0.631      0.528      -1e+10    1.95e+10\n",
      "x113         4.74e+09   7.52e+09      0.631      0.528   -9.99e+09    1.95e+10\n",
      "x114        4.674e+09   7.41e+09      0.631      0.528   -9.85e+09    1.92e+10\n",
      "x115        4.682e+09   7.43e+09      0.631      0.528   -9.87e+09    1.92e+10\n",
      "x116        4.674e+09   7.41e+09      0.631      0.528   -9.85e+09    1.92e+10\n",
      "x117        4.694e+09   7.44e+09      0.631      0.528    -9.9e+09    1.93e+10\n",
      "x118        4.694e+09   7.44e+09      0.631      0.528    -9.9e+09    1.93e+10\n",
      "x119        4.685e+09   7.43e+09      0.631      0.528   -9.88e+09    1.92e+10\n",
      "x120        4.659e+09   7.39e+09      0.631      0.528   -9.82e+09    1.91e+10\n",
      "x121        4.627e+09   7.34e+09      0.631      0.528   -9.76e+09     1.9e+10\n",
      "x122        4.697e+09   7.45e+09      0.631      0.528    -9.9e+09    1.93e+10\n",
      "x123         4.76e+09   7.55e+09      0.631      0.528      -1e+10    1.96e+10\n",
      "x124        4.737e+09   7.51e+09      0.631      0.528   -9.99e+09    1.95e+10\n",
      "x125        4.688e+09   7.44e+09      0.631      0.528   -9.89e+09    1.93e+10\n",
      "x126        4.682e+09   7.43e+09      0.631      0.528   -9.87e+09    1.92e+10\n",
      "x127        4.656e+09   7.38e+09      0.631      0.528   -9.82e+09    1.91e+10\n",
      "x128        4.728e+09    7.5e+09      0.631      0.528   -9.97e+09    1.94e+10\n",
      "x129        4.697e+09   7.45e+09      0.631      0.528    -9.9e+09    1.93e+10\n",
      "x130        4.665e+09    7.4e+09      0.631      0.528   -9.84e+09    1.92e+10\n",
      "x131        4.731e+09    7.5e+09      0.631      0.528   -9.98e+09    1.94e+10\n",
      "x132          4.7e+09   7.45e+09      0.631      0.528   -9.91e+09    1.93e+10\n",
      "x133        4.674e+09   7.41e+09      0.631      0.528   -9.85e+09    1.92e+10\n",
      "x134        4.723e+09   7.49e+09      0.631      0.528   -9.96e+09    1.94e+10\n",
      "x135        4.737e+09   7.51e+09      0.631      0.528   -9.99e+09    1.95e+10\n",
      "x136        4.703e+09   7.46e+09      0.631      0.528   -9.92e+09    1.93e+10\n",
      "x137        4.671e+09   7.41e+09      0.631      0.528   -9.85e+09    1.92e+10\n",
      "x138        4.668e+09    7.4e+09      0.631      0.528   -9.84e+09    1.92e+10\n",
      "x139        4.691e+09   7.44e+09      0.631      0.528   -9.89e+09    1.93e+10\n",
      "x140        4.749e+09   7.53e+09      0.631      0.528      -1e+10    1.95e+10\n",
      "x141        4.731e+09    7.5e+09      0.631      0.528   -9.98e+09    1.94e+10\n",
      "x142        4.737e+09   7.51e+09      0.631      0.528   -9.99e+09    1.95e+10\n",
      "==============================================================================\n",
      "Omnibus:                     5083.358   Durbin-Watson:                   2.002\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            20798.988\n",
      "Skew:                          -0.080   Prob(JB):                         0.00\n",
      "Kurtosis:                       5.334   Cond. No.                     1.17e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 3.6e-27. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "Zero coefficients:\n",
      "Series([], dtype: float64)\n",
      "end\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# ваш код здесь \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Добавляем константу к обучающему набору данных\n",
    "X_train_const = sm.add_constant(X_train_scaled)\n",
    "\n",
    "# Обучаем модель OLS\n",
    "model = sm.OLS(y_train, X_train_const).fit()\n",
    "\n",
    "# Выводим статистическую сводку модели\n",
    "print(model.summary())\n",
    "\n",
    "# Проверяем, какие веса (коэффициенты) равны нулю\n",
    "zero_coeffs = model.params[model.params == 0]\n",
    "print(\"\\nZero coefficients:\")\n",
    "print(zero_coeffs)\n",
    "\n",
    "print(\"end\")\n",
    "\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "fxpo5q8npkutcn6zhdcfpg",
    "execution_id": "3dcf7f85-2aff-4c9e-8a13-b51cf4dd2feb",
    "id": "yLcvGlUZy-Qt"
   },
   "source": [
    "#### 5. [1 балл] Реализуйте один из алгоритмов отбора признаков (Elimination by P-value, Forward elimination, Backward elimination), сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "fcex36oxcnoro0kkxs02",
    "execution_id": "d5a69811-3cca-4026-b03c-b3668b609663"
   },
   "source": [
    "Backward Elimination. В этом методе начинают с полной модели (включая все независимые переменные) \n",
    "и последовательно удаляют наименее важные признаки на основе критерия значимости (например, p-значение).\n",
    "\n",
    "Backward Elimination:\n",
    "Включите все независимые переменные в модель.\n",
    "Обучите модель.\n",
    "Проверьте p-значение всех независимых переменных.\n",
    "Удалите переменную с наибольшим p-значением, если оно больше уровня значимости (например, 0.05).\n",
    "Повторите шаги 2-4, пока все переменные в модели имеют p-значение меньше уровня значимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellId": "izw0w5yhvvnevg2qibit1j",
    "id": "TnrbRbkwy-Qt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love china\n"
     ]
    }
   ],
   "source": [
    "print(\"I love china\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значимые признаки: ['popularity', 'explicit', 'danceability', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature', 'popularity', 'duration_ms', 'explicit', 'danceability', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature', 'track_genre_acoustic', 'track_genre_afrobeat', 'track_genre_alt-rock', 'track_genre_alternative', 'track_genre_ambient', 'track_genre_anime', 'track_genre_black-metal', 'track_genre_bluegrass', 'track_genre_blues', 'track_genre_brazil', 'track_genre_british', 'track_genre_cantopop', 'track_genre_chicago-house', 'track_genre_children', 'track_genre_chill', 'track_genre_classical', 'track_genre_club', 'track_genre_country', 'track_genre_dance', 'track_genre_dancehall', 'track_genre_death-metal', 'track_genre_deep-house', 'track_genre_disco', 'track_genre_disney', 'track_genre_drum-and-bass', 'track_genre_dub', 'track_genre_dubstep', 'track_genre_edm', 'track_genre_electro', 'track_genre_electronic', 'track_genre_emo', 'track_genre_folk', 'track_genre_french', 'track_genre_funk', 'track_genre_garage', 'track_genre_german', 'track_genre_gospel', 'track_genre_goth', 'track_genre_grindcore', 'track_genre_groove', 'track_genre_grunge', 'track_genre_guitar', 'track_genre_happy', 'track_genre_hardstyle', 'track_genre_heavy-metal', 'track_genre_hip-hop', 'track_genre_honky-tonk', 'track_genre_house', 'track_genre_idm', 'track_genre_indian', 'track_genre_indie', 'track_genre_indie-pop', 'track_genre_industrial', 'track_genre_iranian', 'track_genre_j-dance', 'track_genre_j-idol', 'track_genre_j-pop', 'track_genre_j-rock', 'track_genre_jazz', 'track_genre_k-pop', 'track_genre_kids', 'track_genre_latin', 'track_genre_latino', 'track_genre_malay', 'track_genre_mandopop', 'track_genre_metalcore', 'track_genre_minimal-techno', 'track_genre_mpb', 'track_genre_new-age', 'track_genre_opera', 'track_genre_pagode', 'track_genre_party', 'track_genre_piano', 'track_genre_pop', 'track_genre_pop-film', 'track_genre_power-pop', 'track_genre_psych-rock', 'track_genre_punk', 'track_genre_punk-rock', 'track_genre_r-n-b', 'track_genre_reggae', 'track_genre_reggaeton', 'track_genre_rock', 'track_genre_rock-n-roll', 'track_genre_rockabilly', 'track_genre_romance', 'track_genre_sad', 'track_genre_samba', 'track_genre_sertanejo', 'track_genre_show-tunes', 'track_genre_singer-songwriter', 'track_genre_ska', 'track_genre_sleep', 'track_genre_songwriter', 'track_genre_soul', 'track_genre_spanish', 'track_genre_study', 'track_genre_swedish', 'track_genre_synth-pop', 'track_genre_tango', 'track_genre_trance', 'track_genre_trip-hop', 'track_genre_turkish', 'track_genre_world-music']\n"
     ]
    }
   ],
   "source": [
    "def backward_elimination(data, target, significance_level = 0.05):\n",
    "    features = data.columns.tolist()\n",
    "    while (len(features) > 0):\n",
    "        features_with_constant = sm.add_constant(data[features])\n",
    "        p_values = sm.OLS(target, features_with_constant).fit().pvalues[1:]  # исключаем константу\n",
    "        \n",
    "        max_p_value = p_values.max()\n",
    "        if max_p_value > significance_level:\n",
    "            excluded_feature = p_values.idxmax()\n",
    "            features.remove(excluded_feature)\n",
    "        else:\n",
    "            break  # выходим из цикла, если все признаки значимы\n",
    "\n",
    "    return features\n",
    "\n",
    "significant_features = backward_elimination(X_train, y_train)\n",
    "\n",
    "print(\"Значимые признаки:\", significant_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "k6vdrji1ze9nny5dmhpwo"
   },
   "source": [
    "Применение метода Backward Elimination позволяет сократить количество признаков, исключая менее важные из модели.\n",
    "Это может помочь улучшить обобщающую способность модели, уменьшив риск переобучения\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "zr37jmamifims6498o8mxi",
    "id": "df0eQLdNy-Qt"
   },
   "source": [
    "#### 6. [1 балл] Найдите лучший (по RMSE) $\\alpha$ для регрессиии Lasso, используя кросс-валидацию на 5 фолдов. Вы должны выбрать значение из промежутка $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellId": "9m9yzcl6f8d7crjku7aw3x",
    "id": "JPoT3YHqy-Qt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшее значение 𝛼: 0.0001\n",
      "RMSE с этим 𝛼: 0.11278857666616765\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "# Зададим диапазон для 𝛼\n",
    "alphas = np.logspace(-4, 3, 100)  # 100 значений в промежутке [10^-4, 10^3]\n",
    "\n",
    "# Создаем Lasso регрессию с кросс-валидацией на 5 фолдах\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42, max_iter=10000)\n",
    "\n",
    "# Обучаем модель\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Получаем предсказания на тестовых данных\n",
    "y_pred = lasso_cv.predict(X_test_scaled)\n",
    "\n",
    "# Рассчитываем RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Лучшее значение 𝛼: {lasso_cv.alpha_}\")\n",
    "print(f\"RMSE с этим 𝛼: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "fz9ja5r16r86wodhm45ix",
    "execution_id": "db5b89f3-6b2d-4871-926e-7b710c5ae3d0",
    "id": "Q1PKinJUy-Qt"
   },
   "source": [
    "## Градиентный спуск\n",
    "\n",
    "#### 7. [3.5 балла] Имплементируйте  Ridge регрессию для MSE loss, обученную на градиентом спуске.\n",
    "\n",
    "\n",
    "Все вычисления должны быть векторизованы, а циклы Python можно использовать только для итераций градиентного спуска. В качестве критерия остановки необходимо использовать (одновременно):\n",
    "\n",
    "* проверка абсолютной нормы разницы весов на двух соседних итерациях (например, меньше некоторого малого числа порядка $10^{-6}$, заданного параметром `tolerance`);\n",
    "\n",
    "* достижение максимального количества итераций (например, 10000, заданного параметром `max_iter`).\n",
    "\n",
    "Вам необходимо выполнить:\n",
    "\n",
    "* Полный градиентный спуск:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Стохастический градиентный спуск:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ является оценкой градиента по набору объектов, выбранных случайным образом.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "* Adagrad method:\n",
    "\n",
    "$$\n",
    "G_0 = 0, \\\\\n",
    "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
    "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
    "$$\n",
    "\n",
    "Чтобы убедиться, что процесс оптимизации действительно выполняется, мы будем использовать атрибут класса `loss_history`. После вызова метода fit он должен содержать значения функции потерь для всех итераций, начиная с первой (до первого шага по антиградиенту).\n",
    "\n",
    "\n",
    "Вам нужно инициализировать веса случайным вектором из нормального распределения. Ниже приведен шаблон, который должен содержать код, реализующий все варианты моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellId": "bcf1f0nesqr9ckedtyry",
    "id": "oI39UzCLy-Qu"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class MyLinearRegression(BaseEstimator):\n",
    "    def __init__(self, reg_cf=1.0, delta=1.0, gd_type='Momentum', \n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.delta = delta\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = []\n",
    "        self.reg_cf = reg_cf\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.loss_history = []\n",
    "        l, d = X.shape\n",
    "\n",
    "        if self.w0 is None:\n",
    "            self.w = np.zeros(X.shape[1])\n",
    "        else:\n",
    "            self.w = self.w0\n",
    "        \n",
    "        # Previous weights for stopping criterion\n",
    "        w_prev = np.inf * np.ones(d)\n",
    "        \n",
    "        # Initialize h_0 for Momentum and G_0 for Adagrad\n",
    "        h = np.zeros(d)\n",
    "        G = np.zeros(d)\n",
    "        for i in range(self.max_iter):\n",
    "            # Stopping criterion\n",
    "            weight_dist = np.linalg.norm(self.w - w_prev)\n",
    "            if weight_dist < self.tolerance:\n",
    "                break\n",
    "            \n",
    "            # Save previous weights\n",
    "            w_prev = self.w.copy()\n",
    "            \n",
    "            # Compute gradient\n",
    "            if self.gd_type == \"StochasticDescent\":\n",
    "                idx = np.random.choice(l, int(self.delta*l), replace=False)\n",
    "                gradient = self.calc_gradient(X[idx], y[idx])\n",
    "            else:\n",
    "                gradient = self.calc_gradient(X, y)\n",
    "            \n",
    "            # Update weights\n",
    "            if self.gd_type == \"GradientDescent\":\n",
    "                self.w -= self.eta * gradient\n",
    "            elif self.gd_type == \"Momentum\":\n",
    "                h = self.alpha * h + self.eta * gradient\n",
    "                self.w -= h\n",
    "            elif self.gd_type == \"Adagrad\":\n",
    "                G += gradient**2\n",
    "                self.w -= self.eta * gradient / (np.sqrt(G) + self.epsilon)\n",
    "                \n",
    "            # Save loss\n",
    "            self.loss_history.append(self.calc_loss(X, y))\n",
    "            print(self.calc_loss(X, y))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        l, d = X.shape\n",
    "        gradient = -2 * X.T.dot(y - X.dot(self.w)) / l + 2 * self.reg_cf * self.w\n",
    "        return gradient\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        l = len(y)\n",
    "        return np.sum((y - X.dot(self.w)) ** 2) / l + self.reg_cf * np.sum(self.w**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vl0twony16e2i1m9bwhuwt",
    "execution_id": "1dc24052-2bf8-4594-ae9d-36089973ddf4",
    "id": "1QQJEjGVy-Qu"
   },
   "source": [
    "#### 8. [1 балл] Натренируйте и провалидируйте \"ручные\" модели на тех же даннных, сравните качество с моделями из Sklearn и StatsModels. Исследуйте влияние параметров `max_iter` и `alpha` на процесс оптимизации. Соответствует ли оно вашим ожиданиям?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellId": "enwy7ouvu1urdslsovbn6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113186    0.598\n",
       "42819     0.997\n",
       "59311     0.803\n",
       "91368     0.511\n",
       "61000     0.941\n",
       "          ...  \n",
       "91204     0.882\n",
       "102335    0.167\n",
       "107757    0.728\n",
       "86528     0.751\n",
       "86389     0.757\n",
       "Name: energy, Length: 22800, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellId": "xuk2oly8ivt1uc5pn3fs3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63644601, 1.0133417 , 0.52440586, ..., 0.56922391, 0.80848702,\n",
       "       0.70688057])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellId": "qvyagng7pkbfz8arcztpj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4662582115941929\n",
      "0.46008404417301524\n",
      "0.4552155937390991\n",
      "0.45136440040569786\n",
      "0.44830751419253856\n",
      "0.4458723503602588\n",
      "0.44392508696411664\n",
      "0.442361769717093\n",
      "0.4411014870550854\n",
      "0.4400811290761903\n",
      "0.43925135898652995\n",
      "0.4385735133573476\n",
      "0.43801721437764934\n",
      "0.4375585283199904\n",
      "0.43717854339137\n",
      "0.43686226988623256\n",
      "0.43659778828044\n",
      "0.43637558826954714\n",
      "0.4361880550312519\n",
      "0.43602906914816053\n",
      "0.4358936944007596\n",
      "0.43577793359433503\n",
      "0.43567853714668014\n",
      "0.4355928526632565\n",
      "0.435518706412954\n",
      "0.43545430968156695\n",
      "0.43539818456734597\n",
      "0.4353491050048871\n",
      "0.4353060497453192\n",
      "0.4352681647474206\n",
      "0.43523473299575\n",
      "0.43520515019632056\n",
      "0.43517890513701\n",
      "0.4351555637612267\n",
      "0.4351347562065691\n",
      "0.4351161662185112\n",
      "0.4350995224727352\n",
      "0.435084591436399\n",
      "0.4350711714744103\n",
      "0.4350590879663492\n",
      "0.4350481892465936\n",
      "0.43503834321726725\n",
      "0.4350294345129746\n",
      "0.4350213621195989\n",
      "0.435014037367999\n",
      "0.435007382238271\n",
      "0.4350013279221162\n",
      "0.43499581360040623\n",
      "0.4349907854007316\n",
      "0.43498619550593987\n",
      "0.4349820013897209\n",
      "0.43497816515940285\n",
      "0.434974652989473\n",
      "0.4349714346320892\n",
      "0.4349684829930928\n",
      "0.43496577376390283\n",
      "0.43496328510119586\n",
      "0.43496099734755433\n",
      "0.4349588927873193\n",
      "0.4349569554327678\n",
      "0.4349551708364654\n",
      "0.43495352592626957\n",
      "0.43495200885996654\n",
      "0.4349506088969713\n",
      "0.4349493162848796\n",
      "0.4349481221589775\n",
      "0.43494701845307704\n",
      "0.4349459978202673\n",
      "0.43494505356236834\n",
      "RMSE = 0.654\n",
      "R^2 = -5.787\n"
     ]
    }
   ],
   "source": [
    "my_ridge = MyLinearRegression(gd_type='GradientDescent')\n",
    "my_ridge.fit(X_train_scaled, y_train.to_numpy())\n",
    "y_pred = my_ridge.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print(\"RMSE = {:.3f}\".format(mean_squared_error(y_test, y_pred, squared=False)))\n",
    "print(\"R^2 = {:.3f}\".format(r2_score(y_test.to_numpy(), y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellId": "tvvtt7es43g0x0wwx7hsa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>...</th>\n",
       "      <th>track_genre_spanish</th>\n",
       "      <th>track_genre_study</th>\n",
       "      <th>track_genre_swedish</th>\n",
       "      <th>track_genre_synth-pop</th>\n",
       "      <th>track_genre_tango</th>\n",
       "      <th>track_genre_techno</th>\n",
       "      <th>track_genre_trance</th>\n",
       "      <th>track_genre_trip-hop</th>\n",
       "      <th>track_genre_turkish</th>\n",
       "      <th>track_genre_world-music</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84744</th>\n",
       "      <td>28</td>\n",
       "      <td>293960</td>\n",
       "      <td>0</td>\n",
       "      <td>0.640</td>\n",
       "      <td>9</td>\n",
       "      <td>-10.663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.3960</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89295</th>\n",
       "      <td>1</td>\n",
       "      <td>147133</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835</td>\n",
       "      <td>7</td>\n",
       "      <td>-5.620</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.2260</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66688</th>\n",
       "      <td>11</td>\n",
       "      <td>98386</td>\n",
       "      <td>0</td>\n",
       "      <td>0.786</td>\n",
       "      <td>9</td>\n",
       "      <td>-16.516</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.6790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51391</th>\n",
       "      <td>61</td>\n",
       "      <td>232173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.817</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.0698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95123</th>\n",
       "      <td>37</td>\n",
       "      <td>360320</td>\n",
       "      <td>0</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.742</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.5110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76820</th>\n",
       "      <td>49</td>\n",
       "      <td>162613</td>\n",
       "      <td>0</td>\n",
       "      <td>0.554</td>\n",
       "      <td>4</td>\n",
       "      <td>-30.566</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.9150</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110268</th>\n",
       "      <td>30</td>\n",
       "      <td>240062</td>\n",
       "      <td>0</td>\n",
       "      <td>0.689</td>\n",
       "      <td>9</td>\n",
       "      <td>-8.200</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.0910</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103694</th>\n",
       "      <td>0</td>\n",
       "      <td>136306</td>\n",
       "      <td>0</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0</td>\n",
       "      <td>-11.455</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>44</td>\n",
       "      <td>216841</td>\n",
       "      <td>0</td>\n",
       "      <td>0.421</td>\n",
       "      <td>6</td>\n",
       "      <td>-15.191</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.9480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>55</td>\n",
       "      <td>127200</td>\n",
       "      <td>0</td>\n",
       "      <td>0.565</td>\n",
       "      <td>10</td>\n",
       "      <td>-18.678</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91200 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        popularity  duration_ms  explicit  danceability  key  loudness  mode  \\\n",
       "84744           28       293960         0         0.640    9   -10.663     0   \n",
       "89295            1       147133         1         0.835    7    -5.620     0   \n",
       "66688           11        98386         0         0.786    9   -16.516     1   \n",
       "51391           61       232173         0         0.709    0    -5.817     1   \n",
       "95123           37       360320         0         0.786    0    -6.742     1   \n",
       "...            ...          ...       ...           ...  ...       ...   ...   \n",
       "76820           49       162613         0         0.554    4   -30.566     1   \n",
       "110268          30       240062         0         0.689    9    -8.200     1   \n",
       "103694           0       136306         0         0.629    0   -11.455     0   \n",
       "860             44       216841         0         0.421    6   -15.191     1   \n",
       "15795           55       127200         0         0.565   10   -18.678     1   \n",
       "\n",
       "        speechiness  acousticness  instrumentalness  ...  track_genre_spanish  \\\n",
       "84744        0.0396        0.3960          0.024700  ...                    0   \n",
       "89295        0.0694        0.2260          0.000002  ...                    0   \n",
       "66688        0.5730        0.6790          0.000000  ...                    0   \n",
       "51391        0.2450        0.0698          0.000000  ...                    0   \n",
       "95123        0.0456        0.5110          0.000000  ...                    0   \n",
       "...             ...           ...               ...  ...                  ...   \n",
       "76820        0.0502        0.9150          0.000970  ...                    0   \n",
       "110268       0.0759        0.0910          0.914000  ...                    0   \n",
       "103694       0.0523        0.5950          0.000000  ...                    0   \n",
       "860          0.0335        0.9480          0.000000  ...                    0   \n",
       "15795        0.0439        0.9310          0.912000  ...                    0   \n",
       "\n",
       "        track_genre_study  track_genre_swedish  track_genre_synth-pop  \\\n",
       "84744                   0                    0                      0   \n",
       "89295                   0                    0                      0   \n",
       "66688                   0                    0                      0   \n",
       "51391                   0                    0                      0   \n",
       "95123                   0                    0                      0   \n",
       "...                   ...                  ...                    ...   \n",
       "76820                   0                    0                      0   \n",
       "110268                  0                    0                      0   \n",
       "103694                  0                    0                      0   \n",
       "860                     0                    0                      0   \n",
       "15795                   0                    0                      0   \n",
       "\n",
       "        track_genre_tango  track_genre_techno  track_genre_trance  \\\n",
       "84744                   0                   0                   0   \n",
       "89295                   0                   0                   0   \n",
       "66688                   0                   0                   0   \n",
       "51391                   0                   0                   0   \n",
       "95123                   0                   0                   0   \n",
       "...                   ...                 ...                 ...   \n",
       "76820                   0                   0                   0   \n",
       "110268                  0                   0                   1   \n",
       "103694                  0                   0                   0   \n",
       "860                     0                   0                   0   \n",
       "15795                   0                   0                   0   \n",
       "\n",
       "        track_genre_trip-hop  track_genre_turkish  track_genre_world-music  \n",
       "84744                      0                    0                        0  \n",
       "89295                      0                    0                        0  \n",
       "66688                      0                    0                        0  \n",
       "51391                      0                    0                        0  \n",
       "95123                      0                    0                        0  \n",
       "...                      ...                  ...                      ...  \n",
       "76820                      0                    0                        0  \n",
       "110268                     0                    0                        0  \n",
       "103694                     0                    0                        0  \n",
       "860                        0                    0                        0  \n",
       "15795                      0                    0                        0  \n",
       "\n",
       "[91200 rows x 142 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellId": "ojwmbgk4h3kj9ciecsrsd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6855654600401044"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellId": "i3m5d4s3irarawvgn2pko",
    "id": "rIJNcxt_y-Qu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4662582115941929\n",
      "0.46008404417301524\n",
      "0.4552155937390991\n",
      "0.45136440040569786\n",
      "0.44830751419253856\n",
      "0.4458723503602588\n",
      "0.44392508696411664\n",
      "0.442361769717093\n",
      "0.4411014870550854\n",
      "0.4400811290761903\n",
      "0.43925135898652995\n",
      "0.4385735133573476\n",
      "0.43801721437764934\n",
      "0.4375585283199904\n",
      "0.43717854339137\n",
      "0.43686226988623256\n",
      "0.43659778828044\n",
      "0.43637558826954714\n",
      "0.4361880550312519\n",
      "0.43602906914816053\n",
      "0.4358936944007596\n",
      "0.43577793359433503\n",
      "0.43567853714668014\n",
      "0.4355928526632565\n",
      "0.435518706412954\n",
      "0.43545430968156695\n",
      "0.43539818456734597\n",
      "0.4353491050048871\n",
      "0.4353060497453192\n",
      "0.4352681647474206\n",
      "0.43523473299575\n",
      "0.43520515019632056\n",
      "0.43517890513701\n",
      "0.4351555637612267\n",
      "0.4351347562065691\n",
      "0.4351161662185112\n",
      "0.4350995224727352\n",
      "0.435084591436399\n",
      "0.4350711714744103\n",
      "0.4350590879663492\n",
      "0.4350481892465936\n",
      "0.43503834321726725\n",
      "0.4350294345129746\n",
      "0.4350213621195989\n",
      "0.435014037367999\n",
      "0.435007382238271\n",
      "0.4350013279221162\n",
      "0.43499581360040623\n",
      "0.4349907854007316\n",
      "0.43498619550593987\n",
      "0.4349820013897209\n",
      "0.43497816515940285\n",
      "0.434974652989473\n",
      "0.4349714346320892\n",
      "0.4349684829930928\n",
      "0.43496577376390283\n",
      "0.43496328510119586\n",
      "0.43496099734755433\n",
      "0.4349588927873193\n",
      "0.4349569554327678\n",
      "0.4349551708364654\n",
      "0.43495352592626957\n",
      "0.43495200885996654\n",
      "0.4349506088969713\n",
      "0.4349493162848796\n",
      "0.4349481221589775\n",
      "0.43494701845307704\n",
      "0.4349459978202673\n",
      "0.43494505356236834\n",
      "(22800,) (22800,)\n",
      "0.4741118471342967\n",
      "MSE of Sklearn Linear Regression: 0.01481095148096353\n",
      "MSE of Handmade Full Gradient Descent: 0.42713226017631795\n",
      "MSE of Handmade Stochastic Gradient Descent: 0.47674341270481313\n",
      "2.2210385346915976e+18\n",
      "1.4392928001520233e+37\n",
      "9.327005057375462e+55\n",
      "6.044150525252295e+74\n",
      "3.9167723558828087e+93\n",
      "2.538173995454476e+112\n",
      "1.644805121626548e+131\n",
      "1.0658780260824905e+150\n",
      "6.9071767320498e+168\n",
      "4.4760365858295146e+187\n",
      "2.9005922817525438e+206\n",
      "1.8796619338631494e+225\n",
      "1.2180715669143974e+244\n",
      "7.893431874081338e+262\n",
      "5.115156485312017e+281\n",
      "3.314759193037839e+300\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "For max_iter=100 and alpha=0.01, MSE is: nan\n",
      "2.2210385346915976e+18\n",
      "1.4392928001067915e+37\n",
      "9.327005056789198e+55\n",
      "6.044150524682406e+74\n",
      "3.9167723553904543e+93\n",
      "2.5381739950556467e+112\n",
      "1.6448051213164197e+131\n",
      "1.0658780258480232e+150\n",
      "6.907176730313293e+168\n",
      "4.476036584563576e+187\n",
      "2.9005922808410184e+206\n",
      "1.8796619332133694e+225\n",
      "1.2180715664550341e+244\n",
      "7.893431870856544e+262\n",
      "5.115156483061441e+281\n",
      "3.3147591914753065e+300\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "For max_iter=100 and alpha=0.05, MSE is: nan\n",
      "2.2210385346915976e+18\n",
      "1.4392928000502523e+37\n",
      "9.327005056056653e+55\n",
      "6.044150523970417e+74\n",
      "3.9167723547752365e+93\n",
      "2.5381739945572385e+112\n",
      "1.644805120928814e+131\n",
      "1.06587802555497e+150\n",
      "6.907176728142854e+168\n",
      "4.4760365829811765e+187\n",
      "2.9005922797016772e+206\n",
      "1.8796619324011797e+225\n",
      "1.218071565880865e+244\n",
      "7.893431866825648e+262\n",
      "5.1151564802484216e+281\n",
      "3.3147591895221526e+300\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "For max_iter=100 and alpha=0.1, MSE is: nan\n",
      "2.2210385346915976e+18\n",
      "1.4392927995979353e+37\n",
      "9.327005050194161e+55\n",
      "6.044150518271786e+74\n",
      "3.916772349851417e+93\n",
      "2.5381739905688176e+112\n",
      "1.6448051178273072e+131\n",
      "1.0658780232101413e+150\n",
      "6.907176710777026e+168\n",
      "4.476036570321046e+187\n",
      "2.900592270585961e+206\n",
      "1.879661925903293e+225\n",
      "1.218071561287249e+244\n",
      "7.89343183457714e+262\n",
      "5.115156457742929e+281\n",
      "3.314759173896333e+300\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m al \u001b[38;5;129;01min\u001b[39;00m alphas:\n\u001b[1;32m     35\u001b[0m     model \u001b[38;5;241m=\u001b[39m MyLinearRegression(gd_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMomentum\u001b[39m\u001b[38;5;124m'\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39mit, alpha\u001b[38;5;241m=\u001b[39mal)\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     38\u001b[0m     mse \u001b[38;5;241m=\u001b[39m ((predictions \u001b[38;5;241m-\u001b[39m y_test) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[18], line 63\u001b[0m, in \u001b[0;36mMyLinearRegression.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Save loss\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_history\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_loss(X, y))\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[18], line 78\u001b[0m, in \u001b[0;36mMyLinearRegression.calc_loss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m     77\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum((y \u001b[38;5;241m-\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m l \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_cf \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:1612\u001b[0m, in \u001b[0;36mDataFrame.dot\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(\n\u001b[1;32m   1609\u001b[0m         np\u001b[38;5;241m.\u001b[39mdot(lvals, rvals), index\u001b[38;5;241m=\u001b[39mleft\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mother\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   1610\u001b[0m     )\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Series):\n\u001b[0;32m-> 1612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_sliced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvals, (np\u001b[38;5;241m.\u001b[39mndarray, Index)):\n\u001b[1;32m   1614\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(lvals, rvals)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:472\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     data \u001b[38;5;241m=\u001b[39m sanitize_array(data, index, dtype, copy)\n\u001b[0;32m--> 472\u001b[0m     manager \u001b[38;5;241m=\u001b[39m \u001b[43mget_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode.data_manager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    474\u001b[0m         data \u001b[38;5;241m=\u001b[39m SingleBlockManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_config/config.py:262\u001b[0m, in \u001b[0;36mCallableDynamicDoc.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__doc_tmpl__ \u001b[38;5;241m=\u001b[39m doc_tmpl\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__func__\u001b[39m \u001b[38;5;241m=\u001b[39m func\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__func__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# error: Signature of \"__doc__\" incompatible with supertype \"object\"\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Значения MSE для моделей Sklearn\n",
    "mse_sklearn_linear = 0.12170025259202846 ** 2\n",
    "mse_sklearn_ridge = 0.12170025363233108 ** 2\n",
    "mse_sklearn_lasso = 0.14798373613042706 ** 2\n",
    "mse_sklearn_elasticnet = 0.12268881141903015 ** 2\n",
    "\n",
    "# Обучение и валидация \"ручных\" моделей\n",
    "# Предполагаем, что у вас есть класс MyLinearRegression с различными типами градиентного спуска\n",
    "\n",
    "model_full_gd = MyLinearRegression(gd_type='GradientDescent')\n",
    "model_full_gd.fit(X_train_scaled, y_train)\n",
    "predictions_full_gd = model_full_gd.predict(X_test_scaled)\n",
    "print(predictions_full_gd.shape, y_test.shape)\n",
    "mse_full_gd = ((predictions_full_gd - y_test) ** 2).mean()\n",
    "\n",
    "# Аналогично для других типов градиентного спуска, например:\n",
    "model_stochastic_gd = MyLinearRegression(gd_type='Stochastic')\n",
    "model_stochastic_gd.fit(X_train_scaled, y_train)\n",
    "predictions_stochastic_gd = model_stochastic_gd.predict(X_test_scaled)\n",
    "mse_stochastic_gd = ((predictions_stochastic_gd - y_test) ** 2).mean()\n",
    "\n",
    "# Вывод MSE для различных моделей\n",
    "print(f\"MSE of Sklearn Linear Regression: {mse_sklearn_linear}\")\n",
    "print(f\"MSE of Handmade Full Gradient Descent: {mse_full_gd}\")\n",
    "print(f\"MSE of Handmade Stochastic Gradient Descent: {mse_stochastic_gd}\")\n",
    "# ... и так далее для других моделей\n",
    "\n",
    "# Исследование влияния max_iter и alpha на процесс оптимизации\n",
    "iterations = [100, 500, 1000, 5000]\n",
    "alphas = [0.01, 0.05, 0.1, 0.5]\n",
    "\n",
    "\n",
    "for it in iterations:\n",
    "    for al in alphas:\n",
    "        model = MyLinearRegression(gd_type='Momentum', max_iter=it, alpha=al)\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        mse = ((predictions - y_test) ** 2).mean()\n",
    "        print(f\"For max_iter={it} and alpha={al}, MSE is: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "dfbc3jm30jaeh15ejdgfz"
   },
   "outputs": [],
   "source": [
    "print(\"dam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "i2b2x2jge8omcx0kg2stq",
    "id": "bqYtVqv-y-Qu"
   },
   "source": [
    "#### 9. [1 балл] Постройте графики (там же) зависимости значения функции потерь от номера итерации для всех моделей (полного градиентого спуска, стохастического гс, Momentum и Adagrad). Сделайте выводы о скорости сходимости различных модификаций градиентного спуска.\n",
    "\n",
    "\n",
    "Не забывайте о том, как должен выглядеть *красивый* график!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellId": "esgmv9xukhs8xdy4yr56ks",
    "id": "Xbwhu8BSy-Qu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellId": "uus3kigbpgqb258ii074vn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2210385346915976e+18\n",
      "1.439292800163331e+37\n",
      "9.327005057522061e+55\n",
      "6.044150525394827e+74\n",
      "3.916772356005945e+93\n",
      "2.5381739955542066e+112\n",
      "1.6448051217040998e+131\n",
      "1.0658780261411119e+150\n",
      "6.90717673248393e+168\n",
      "4.476036586145994e+187\n",
      "2.90059228198044e+206\n",
      "1.8796619340255567e+225\n",
      "1.218071567029194e+244\n",
      "7.893431874887312e+262\n",
      "5.1151564858745507e+281\n",
      "3.3147591934284776e+300\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_full_gd \u001b[38;5;241m=\u001b[39m \u001b[43mMyLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgd_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGradientDescent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model_sgd = LinReg(gd_type='StochasticDescent').fit(X_train, y_train)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model_momentum = MyLinearRegression(gd_type='Momentum').fit(X_train, y_train)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_adagrad \u001b[38;5;241m=\u001b[39m MyLinearRegression(gd_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdagrad\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "Cell \u001b[0;32mIn[18], line 49\u001b[0m, in \u001b[0;36mMyLinearRegression.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     47\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_gradient(X[idx], y[idx])\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgd_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradientDescent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[18], line 73\u001b[0m, in \u001b[0;36mMyLinearRegression.calc_gradient\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m     72\u001b[0m     l, d \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 73\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241m.\u001b[39mdot(y \u001b[38;5;241m-\u001b[39m X\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw)) \u001b[38;5;241m/\u001b[39m l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_cf \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gradient\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3698\u001b[0m, in \u001b[0;36mDataFrame.T\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3696\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   3697\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mT\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m-> 3698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3689\u001b[0m, in \u001b[0;36mDataFrame.transpose\u001b[0;34m(self, copy, *args)\u001b[0m\n\u001b[1;32m   3684\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_from_arrays(\n\u001b[1;32m   3685\u001b[0m         new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m   3686\u001b[0m     )\n\u001b[1;32m   3688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3689\u001b[0m     new_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m   3690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   3691\u001b[0m         new_arr \u001b[38;5;241m=\u001b[39m new_arr\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:11739\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  11666\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  11667\u001b[0m \u001b[38;5;124;03mReturn a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[1;32m  11668\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11736\u001b[0m \u001b[38;5;124;03m       ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[1;32m  11737\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  11738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m> 11739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/internals/managers.py:1770\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1768\u001b[0m             arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1770\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/internals/managers.py:1836\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[0;34m(self, dtype, na_value)\u001b[0m\n\u001b[1;32m   1834\u001b[0m         arr \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mget_values(dtype)\n\u001b[1;32m   1835\u001b[0m     result[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m-> 1836\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome items were not contained in blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_full_gd = MyLinearRegression(gd_type='GradientDescent').fit(X_train, y_train)\n",
    "# model_sgd = LinReg(gd_type='StochasticDescent').fit(X_train, y_train)\n",
    "# model_momentum = MyLinearRegression(gd_type='Momentum').fit(X_train, y_train)\n",
    "model_adagrad = MyLinearRegression(gd_type='Adagrad').fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellId": "egwilxb3j6drs97ad24rr"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_momentum' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m loss_history_full_gd \u001b[38;5;241m=\u001b[39m model_full_gd\u001b[38;5;241m.\u001b[39mloss_history\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# loss_history_sgd = model_sgd.loss_history\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m loss_history_momentum \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_momentum\u001b[49m\u001b[38;5;241m.\u001b[39mloss_history\n\u001b[1;32m      4\u001b[0m loss_history_adagrad \u001b[38;5;241m=\u001b[39m model_adagrad\u001b[38;5;241m.\u001b[39mloss_history\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_momentum' is not defined"
     ]
    }
   ],
   "source": [
    "loss_history_full_gd = model_full_gd.loss_history\n",
    "# loss_history_sgd = model_sgd.loss_history\n",
    "loss_history_momentum = model_momentum.loss_history\n",
    "loss_history_adagrad = model_adagrad.loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellId": "u4xfyo6f8y0ariqyenta3m"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_history_momentum' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(loss_history_full_gd, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFull Gradient Descent\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# plt.plot(loss_history_sgd, label='Stochastic Gradient Descent', color='green')\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mloss_history_momentum\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMomentum\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(loss_history_adagrad, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdagrad\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurple\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss History for Different Gradient Descent Types\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_history_momentum' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Отображаем истории потерь для каждой модели\n",
    "plt.plot(loss_history_full_gd, label='Full Gradient Descent', color='blue')\n",
    "# plt.plot(loss_history_sgd, label='Stochastic Gradient Descent', color='green')\n",
    "plt.plot(loss_history_momentum, label='Momentum', color='red')\n",
    "plt.plot(loss_history_adagrad, label='Adagrad', color='purple')\n",
    "\n",
    "plt.title('Loss History for Different Gradient Descent Types')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"end my life\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cellId": "seav5f88eu0664s07zduo5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "kz7kl377pbqty0ptmncf38"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "notebookId": "a72ff01e-e243-474b-b09d-7eb864a8d53b",
  "notebookPath": "hw2_version1 (2).ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
